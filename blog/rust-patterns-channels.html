<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Rust Patterns #20: Channels - Telex</title>
  <meta name="description" content="Instead of sharing memory and locking, send data between threads through channels. Decoupled, composable, and deadlock-free by design.">
  <link rel="stylesheet" href="../style.css">
  <link rel="icon" type="image/png" href="../assets/telex-tui.png">
</head>
<body>
  <header>
    <div class="container">
      <a href="/" class="logo-link">
        <img src="../assets/telex-tui.png" alt="Telex logo">
        Telex
      </a>
      <nav>
        <a href="https://telex-tui.github.io/telex-tui/">Book</a>
        <a href="/blog/">Blog</a>
        <a href="https://github.com/telex-tui/telex-tui">GitHub</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <article class="post">
      <h1>Rust Patterns That Matter #20: Channels &mdash; Message Passing</h1>
      <div class="post-meta">August 2026</div>
      <p class="series-nav">Post 20 of 22 in <a href="#series-index">Rust Patterns That Matter</a>.</p>

      <p>
        The previous post covered <code>Arc&lt;Mutex&lt;T&gt;&gt;</code> for shared state
        across threads. It works, but shared mutable state is inherently complex: lock
        ordering, potential deadlocks, contention. Channels offer an alternative: instead
        of sharing memory and protecting it with locks, send data between threads. No shared
        state, no locks, no deadlocks.
      </p>

      <h2>The motivation</h2>

      <p>
        You have a producer thread generating work items and a consumer thread processing
        them. With shared state:
      </p>

      <pre><code><span class="kw">use</span> std::sync::{<span class="ty">Arc</span>, <span class="ty">Mutex</span>, <span class="ty">Condvar</span>};
<span class="kw">use</span> std::collections::<span class="ty">VecDeque</span>;

<span class="kw">let</span> queue = <span class="ty">Arc</span>::<span class="fn">new</span>((<span class="ty">Mutex</span>::<span class="fn">new</span>(<span class="ty">VecDeque</span>::<span class="fn">new</span>()), <span class="ty">Condvar</span>::<span class="fn">new</span>()));
<span class="cm">// Producer: lock, push, notify</span>
<span class="cm">// Consumer: lock, check if empty, wait on condvar, pop</span>
<span class="cm">// ... 30+ lines of careful locking</span></code></pre>

      <p>
        This works but requires managing a mutex, a condition variable, and the wake-up
        logic manually. It's error-prone and hard to extend.
      </p>

      <h2>The pattern: channels</h2>

      <pre><code><span class="kw">use</span> std::sync::mpsc;

<span class="kw">let</span> (tx, rx) = mpsc::<span class="fn">channel</span>();

<span class="cm">// Producer thread</span>
std::thread::<span class="fn">spawn</span>(<span class="kw">move</span> || {
    <span class="kw">for</span> i <span class="kw">in</span> <span class="num">0</span>..<span class="num">5</span> {
        tx.<span class="fn">send</span>(<span class="mac">format!</span>(<span class="str">"item {i}"</span>)).<span class="fn">unwrap</span>();
    }
});

<span class="cm">// Consumer (main thread)</span>
<span class="kw">for</span> msg <span class="kw">in</span> rx {
    <span class="mac">println!</span>(<span class="str">"received: {msg}"</span>);
}</code></pre>

      <p>
        <code>channel()</code> returns a <code>(Sender&lt;T&gt;, Receiver&lt;T&gt;)</code>
        pair. The sender sends values. The receiver blocks until a value arrives. When the
        sender is dropped, the receiver's iterator ends. No locks, no condition variables,
        no shared state.
      </p>

      <h2>Multiple producers</h2>

      <p>
        <code>mpsc</code> stands for "multiple producer, single consumer." Clone the sender
        to share it among multiple producers:
      </p>

      <pre><code><span class="kw">let</span> (tx, rx) = mpsc::<span class="fn">channel</span>();

<span class="kw">for</span> id <span class="kw">in</span> <span class="num">0</span>..<span class="num">4</span> {
    <span class="kw">let</span> tx = tx.<span class="fn">clone</span>();
    std::thread::<span class="fn">spawn</span>(<span class="kw">move</span> || {
        tx.<span class="fn">send</span>(<span class="mac">format!</span>(<span class="str">"from worker {id}"</span>)).<span class="fn">unwrap</span>();
    });
}

<span class="fn">drop</span>(tx); <span class="cm">// drop the original sender so rx knows when all senders are gone</span>

<span class="kw">for</span> msg <span class="kw">in</span> rx {
    <span class="mac">println!</span>(<span class="str">"{msg}"</span>);
}</code></pre>

      <p>
        Each worker gets its own cloned sender. The receiver collects messages from all
        workers. When all senders are dropped, the receiver loop ends.
      </p>

      <h2>Bounded vs unbounded</h2>

      <p>
        <code>channel()</code> is unbounded: the producer never blocks, and the internal
        buffer grows without limit. If the producer is faster than the consumer, memory
        usage grows indefinitely.
      </p>
      <p>
        <code>sync_channel(n)</code> is bounded: the buffer holds at most <code>n</code>
        messages. If it's full, the producer blocks until the consumer takes a message.
        This provides backpressure:
      </p>

      <pre><code><span class="kw">let</span> (tx, rx) = mpsc::<span class="fn">sync_channel</span>(<span class="num">10</span>); <span class="cm">// buffer of 10</span>

<span class="cm">// Producer blocks when buffer is full</span>
<span class="cm">// Consumer processes at its own pace</span>
<span class="cm">// Memory usage is bounded</span></code></pre>

      <p>
        For most production systems, bounded channels are the right default. They prevent
        unbounded memory growth and naturally balance producer/consumer speeds.
      </p>

      <h2><code>crossbeam-channel</code></h2>

      <p>
        The standard library's <code>mpsc</code> covers basic cases. For more advanced
        patterns, <code>crossbeam-channel</code> provides:
      </p>
      <ul>
        <li><strong>MPMC</strong> (multiple producer, multiple consumer) &mdash; multiple
          threads can receive from the same channel</li>
        <li><strong><code>select!</code></strong> &mdash; wait on multiple channels
          simultaneously, acting on whichever has data first</li>
        <li><strong>Timeouts</strong> &mdash; <code>recv_timeout</code> for non-blocking
          waits</li>
        <li><strong>Better performance</strong> &mdash; crossbeam channels are generally
          faster than <code>std::sync::mpsc</code></li>
      </ul>

      <pre><code><span class="kw">use</span> crossbeam_channel::{<span class="fn">select</span>, <span class="fn">unbounded</span>};

<span class="kw">let</span> (tx_work, rx_work) = <span class="fn">unbounded</span>();
<span class="kw">let</span> (tx_quit, rx_quit) = <span class="fn">unbounded</span>();

<span class="kw">loop</span> {
    <span class="fn">select!</span> {
        <span class="fn">recv</span>(rx_work) -&gt; msg =&gt; {
            <span class="kw">let</span> msg = msg.<span class="fn">unwrap</span>();
            <span class="mac">println!</span>(<span class="str">"work: {msg}"</span>);
        }
        <span class="fn">recv</span>(rx_quit) -&gt; _ =&gt; {
            <span class="mac">println!</span>(<span class="str">"shutting down"</span>);
            <span class="kw">break</span>;
        }
    }
}</code></pre>

      <h2>Telex's use of channels</h2>

      <p>
        Telex uses channels for external event integration. When a background task (file
        watcher, network listener) needs to send events into the UI loop, it sends them
        through a channel. The main loop receives from the channel on each tick, triggering
        re-renders. See
        <a href="what-the-real-world-taught-us.html">Designing a TUI Framework &mdash;
        Part 2</a> for the full story on ports and channels.
      </p>

      <h2>When to use channels vs shared state</h2>

      <ul>
        <li><strong>Channels:</strong> producer/consumer patterns, work queues, pipelines,
          event systems, decoupled components. When the data flows in one direction.</li>
        <li><strong>Shared state (<code>Arc&lt;Mutex&lt;T&gt;&gt;</code>):</strong> when
          multiple threads need low-latency access to the same data structure (a shared
          cache, a configuration map). When the data doesn't flow &mdash; it's consulted
          in place.</li>
      </ul>
      <blockquote>
        "Don't communicate by sharing memory; share memory by communicating."
        &mdash; Go proverb, equally applicable in Rust
      </blockquote>
      <p>
        When in doubt, start with channels. They're easier to reason about, can't deadlock
        (no locks to hold), and naturally decouple components. Move to shared state only
        when channels introduce unacceptable latency or awkward serialization of access.
      </p>

      <nav class="series-index" id="series-index">
        <h2>Series index</h2>
        <ol>
          <li><a href="rust-patterns-newtype.html">#1: Newtype</a></li>
          <li><a href="rust-patterns-from-into.html">#2: From / Into Conversions</a></li>
          <li><a href="rust-patterns-error-handling.html">#3: Error Handling</a></li>
          <li><a href="rust-patterns-interior-mutability.html">#4: Interior Mutability</a></li>
          <li><a href="rust-patterns-shared-ownership.html">#5: Shared Ownership</a></li>
          <li><a href="rust-patterns-rc-refcell.html">#6: The Combo &mdash; Rc&lt;RefCell&lt;T&gt;&gt;</a></li>
          <li><a href="rust-patterns-split-borrows.html">#7: Split Borrows</a></li>
          <li><a href="rust-patterns-index-based-design.html">#8: Index-Based Design</a></li>
          <li><a href="rust-patterns-drop-raii.html">#9: Drop and RAII</a></li>
          <li><a href="rust-patterns-lifetime-annotations.html">#10: Lifetime Annotations</a></li>
          <li><a href="rust-patterns-cow.html">#11: Cow &mdash; Borrow or Own</a></li>
          <li><a href="rust-patterns-iterators.html">#12: Custom Iterators</a></li>
          <li><a href="rust-patterns-static-clone.html">#13: &rsquo;static + Clone</a></li>
          <li><a href="rust-patterns-enum-dispatch.html">#14: Enum Dispatch vs Trait Objects</a></li>
          <li><a href="rust-patterns-closure-traits.html">#15: Fn, FnMut, FnOnce</a></li>
          <li><a href="rust-patterns-storing-closures.html">#16: Storing and Returning Closures</a></li>
          <li><a href="rust-patterns-builder.html">#17: Builder Pattern</a></li>
          <li><a href="rust-patterns-typestate.html">#18: Typestate</a></li>
          <li><a href="rust-patterns-arc-mutex.html">#19: Arc&lt;Mutex&lt;T&gt;&gt; vs Arc&lt;RwLock&lt;T&gt;&gt;</a></li>
          <li><strong>#20: Channels &mdash; Message Passing</strong></li>
          <li><a href="rust-patterns-pin-futures.html">#21: Pin and Boxing Futures</a></li>
          <li><a href="rust-patterns-send-sync-async.html">#22: Send / Sync in Async</a></li>
        </ol>
      </nav>
    </article>
  </main>

  <footer>
    <div class="container">
      MIT &mdash; Copyright &copy; 2025 Mark Branion
    </div>
  </footer>
</body>
</html>
