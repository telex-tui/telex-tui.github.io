<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Rust Patterns #17: Arc Mutex vs Arc RwLock - Telex</title>
  <meta name="description" content="The thread-safe versions of Rc and RefCell. Arc for sharing across threads, Mutex or RwLock for mutation. Default to Mutex — reach for RwLock when reads dominate.">
  <link rel="stylesheet" href="../style.css">
  <link rel="icon" type="image/png" href="../assets/telex-tui.png">
</head>
<body>
  <header>
    <div class="container">
      <a href="/" class="logo-link">
        <img src="../assets/telex-tui.png" alt="Telex logo">
        Telex
      </a>
      <nav>
        <a href="https://telex-tui.github.io/telex-tui/">Book</a>
        <a href="/blog/">Blog</a>
        <a href="https://github.com/telex-tui/telex-tui">GitHub</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <article class="post">
      <h1>Rust Patterns That Matter #17: <code>Arc&lt;Mutex&lt;T&gt;&gt;</code> vs <code>Arc&lt;RwLock&lt;T&gt;&gt;</code></h1>
      <div class="post-meta">August 2026</div>
      <p class="series-nav">Post 17 of 20 in <a href="#series-index">Rust Patterns That Matter</a>.</p>

      <p>
        You have data that multiple threads need to read and write. You reach for
        <code>Rc&lt;RefCell&lt;T&gt;&gt;</code> because that's what worked in
        <a href="rust-patterns-rc-refcell.html">#3</a>. The compiler says no:
        <code>Rc</code> is not <code>Send</code>, <code>RefCell</code> is not
        <code>Sync</code>. The thread-safe equivalents are <code>Arc</code> and
        <code>Mutex</code>. They wrap your data the same way, but they use atomic
        operations and OS locks so multiple threads can share safely.
      </p>

      <h2>The motivation</h2>

      <pre><code><span class="kw">use</span> std::rc::<span class="ty">Rc</span>;
<span class="kw">use</span> std::cell::<span class="ty">RefCell</span>;

<span class="kw">let</span> counter = <span class="ty">Rc</span>::<span class="fn">new</span>(<span class="ty">RefCell</span>::<span class="fn">new</span>(<span class="num">0</span>));
<span class="kw">let</span> c = <span class="ty">Rc</span>::<span class="fn">clone</span>(&amp;counter);

std::thread::<span class="fn">spawn</span>(<span class="kw">move</span> || {
    *c.<span class="fn">borrow_mut</span>() += <span class="num">1</span>;
});</code></pre>

      <pre><code><span class="cm">error[E0277]: `Rc&lt;RefCell&lt;i32&gt;&gt;` cannot be sent between threads safely
              the trait `Send` is not implemented for `Rc&lt;RefCell&lt;i32&gt;&gt;`</span></code></pre>

      <p>
        <code>Rc</code> uses a plain integer for its reference count. If two threads
        increment it at the same time, the count ends up wrong, and memory gets freed too
        early or leaked. <code>RefCell</code> tracks borrows with a plain integer too.
        Same problem. Neither is safe to share across threads.
      </p>

      <h2>The parallel to Part I</h2>

      <p>
        The mapping is direct:
      </p>
      <ul>
        <li><code>Rc</code> &rarr; <code>Arc</code> (Atomic Reference Counted)</li>
        <li><code>RefCell</code> &rarr; <code>Mutex</code> (mutual exclusion lock)</li>
        <li><code>Rc&lt;RefCell&lt;T&gt;&gt;</code> &rarr; <code>Arc&lt;Mutex&lt;T&gt;&gt;</code></li>
      </ul>
      <p>
        <code>Arc</code> uses atomic operations for the reference count, making it safe
        to clone across threads. <code>Mutex</code> uses OS-level locking to ensure only
        one thread accesses the data at a time.
      </p>

      <h2>Inside Arc: atomic reference counting</h2>

      <p>
        <code>Arc</code> stores its reference counts as <code>AtomicUsize</code>
        values. Cloning an <code>Arc</code> calls
        <code>fetch_add(1, Ordering::Relaxed)</code> on the strong count — a single
        atomic read-modify-write instruction. Dropping calls
        <code>fetch_sub(1, Ordering::Release)</code>, and if the count reaches zero,
        an <code>Acquire</code> fence ensures all previous writes are visible before
        the data is dropped.
      </p>
      <p>
        Why <code>Relaxed</code> for increment but <code>Release</code>/<code>Acquire</code>
        for the final decrement? Incrementing doesn't synchronize data — it just
        keeps the allocation alive. But the drop path needs to see all writes made
        through other <code>Arc</code> clones before it frees the memory. The
        <code>Release</code> on decrement and <code>Acquire</code> fence on the
        zero-reaching thread establish that happens-before relationship.
      </p>
      <p>
        This is a concrete example of memory ordering in Rust. Most code never
        touches atomics directly — <code>Arc</code> encapsulates the ordering
        decisions. But understanding them explains why <code>Arc::clone</code> is
        cheap (one relaxed atomic op, no memory fence) while the final drop is
        slightly more expensive (release store plus acquire fence).
      </p>

      <h2>The pattern: <code>Arc&lt;Mutex&lt;T&gt;&gt;</code></h2>

      <pre><code><span class="kw">use</span> std::sync::{<span class="ty">Arc</span>, <span class="ty">Mutex</span>};

<span class="kw">let</span> counter = <span class="ty">Arc</span>::<span class="fn">new</span>(<span class="ty">Mutex</span>::<span class="fn">new</span>(<span class="num">0</span>));
<span class="kw">let</span> <span class="kw">mut</span> handles = <span class="fn">vec!</span>[];

<span class="kw">for</span> _ <span class="kw">in</span> <span class="num">0</span>..<span class="num">10</span> {
    <span class="kw">let</span> counter = <span class="ty">Arc</span>::<span class="fn">clone</span>(&amp;counter);
    handles.<span class="fn">push</span>(std::thread::<span class="fn">spawn</span>(<span class="kw">move</span> || {
        <span class="kw">let</span> <span class="kw">mut</span> num = counter.<span class="fn">lock</span>().<span class="fn">unwrap</span>();
        *num += <span class="num">1</span>;
    }));
}

<span class="kw">for</span> h <span class="kw">in</span> handles {
    h.<span class="fn">join</span>().<span class="fn">unwrap</span>();
}

<span class="mac">println!</span>(<span class="str">"{}"</span>, *counter.<span class="fn">lock</span>().<span class="fn">unwrap</span>()); <span class="cm">// 10</span></code></pre>

      <p>
        <code>Arc::clone</code> gives each thread its own handle to the shared data.
        <code>.lock()</code> acquires the mutex, returning a guard that dereferences to
        <code>&amp;mut T</code>. When the guard is dropped, the lock is released. Only one
        thread holds the lock at a time.
      </p>

      <h2>Mutex poisoning</h2>

      <p>
        <code>.lock()</code> returns <code>Result&lt;MutexGuard, PoisonError&gt;</code>.
        If a thread panics while holding the lock, the mutex becomes "poisoned" &mdash;
        the data inside might be in an inconsistent state.
      </p>
      <p>
        In most programs, <code>.lock().unwrap()</code> is correct. If another thread
        panicked, your data may be corrupt, and propagating the panic is reasonable. If
        you want to recover from a poisoned mutex (because you can validate or reset the
        data), use <code>.lock().unwrap_or_else(|e| e.into_inner())</code>.
      </p>

      <h2><code>Arc&lt;RwLock&lt;T&gt;&gt;</code></h2>

      <p>
        <code>RwLock</code> (read-write lock) allows multiple simultaneous readers or one
        exclusive writer:
      </p>

      <pre><code><span class="kw">use</span> std::sync::{<span class="ty">Arc</span>, <span class="ty">RwLock</span>};

<span class="kw">let</span> data = <span class="ty">Arc</span>::<span class="fn">new</span>(<span class="ty">RwLock</span>::<span class="fn">new</span>(<span class="fn">vec!</span>[<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>]));

<span class="cm">// Multiple readers — concurrent</span>
<span class="kw">let</span> r1 = data.<span class="fn">read</span>().<span class="fn">unwrap</span>();
<span class="kw">let</span> r2 = data.<span class="fn">read</span>().<span class="fn">unwrap</span>();
<span class="mac">println!</span>(<span class="str">"{} {}"</span>, r1.<span class="fn">len</span>(), r2.<span class="fn">len</span>()); <span class="cm">// both held simultaneously</span>
<span class="fn">drop</span>(r1);
<span class="fn">drop</span>(r2);

<span class="cm">// One writer — exclusive</span>
<span class="kw">let</span> <span class="kw">mut</span> w = data.<span class="fn">write</span>().<span class="fn">unwrap</span>();
w.<span class="fn">push</span>(<span class="num">4</span>); <span class="cm">// no readers or other writers can access while this is held</span></code></pre>

      <p>
        This follows the same rule as the borrow checker (many <code>&amp;T</code> or one
        <code>&amp;mut T</code>), but checked at runtime with locks instead of at compile
        time.
      </p>

      <h2>Mutex vs RwLock</h2>

      <p>
        <code>RwLock</code> sounds strictly better &mdash; it lets multiple threads read
        at the same time. But every lock and unlock costs more because the lock has to
        track how many readers are active. If threads aren't fighting over the lock much,
        or reads and writes are roughly balanced, <code>Mutex</code> is faster.
      </p>
      <p>
        <strong>Default to <code>Mutex</code>.</strong> Reach for <code>RwLock</code> when:
      </p>
      <ul>
        <li>Reads vastly outnumber writes (100:1 or more)</li>
        <li>Read operations are slow enough that concurrent reads matter</li>
        <li>You've measured contention and confirmed that readers are blocking each other</li>
      </ul>
      <p>
        For most applications, <code>Mutex</code> is the right choice.
      </p>

      <h2>What does this cost?</h2>

      <p>
        <code>Arc</code>: one heap allocation at creation, one atomic
        increment/decrement per clone/drop. On x86, atomic operations on naturally
        aligned words are lock-free — they don't take an OS lock. The cost is a
        few nanoseconds per operation, mainly from cache-line synchronization between
        cores. <code>Mutex</code>: an uncontended lock/unlock is typically 15–25ns
        on modern x86 (a single atomic compare-and-swap). Contended locks involve
        OS futex calls and thread parking, which cost microseconds. Without
        <code>Arc&lt;Mutex&lt;T&gt;&gt;</code>, shared mutable state across threads
        would require unsafe — raw atomic operations, manual memory fences, and
        hand-verified synchronization protocols. <code>Mutex</code> is that protocol,
        audited and correct.
      </p>

      <h2>When to use it</h2>

      <ul>
        <li>A counter that multiple threads increment (like a request counter)</li>
        <li>A <code>HashMap</code> that threads read from and write to (like a lookup cache)</li>
        <li>A config struct that the main thread updates and worker threads read</li>
        <li>Any data that more than one thread needs to read or write</li>
      </ul>
      <p>
        When not to: if you can avoid sharing data between threads entirely by sending
        messages instead (<a href="rust-patterns-channels.html">#18</a>), do that. Locks
        are safe in Rust, but they're still harder to follow than messages flowing in one
        direction. <code>Arc&lt;Mutex&lt;T&gt;&gt;</code> won't cause data races, but if
        one thread grabs lock A then tries to grab lock B while another thread holds B and
        waits on A, you get a deadlock.
      </p>

      <nav class="series-index" id="series-index">
        <h2>Series index</h2>
        <ol>
          <li><a href="rust-patterns-interior-mutability.html">#1: Interior Mutability</a></li>
          <li><a href="rust-patterns-shared-ownership.html">#2: Shared Ownership</a></li>
          <li><a href="rust-patterns-rc-refcell.html">#3: The Combo &mdash; Rc&lt;RefCell&lt;T&gt;&gt;</a></li>
          <li><a href="rust-patterns-split-borrows.html">#4: Split Borrows</a></li>
          <li><a href="rust-patterns-index-based-design.html">#5: Index-Based Design</a></li>
          <li><a href="rust-patterns-cow.html">#6: Cow &mdash; Borrow or Own</a></li>
          <li><a href="rust-patterns-lifetime-annotations.html">#7: Lifetime Annotations</a></li>
          <li><a href="rust-patterns-static-clone.html">#8: &rsquo;static + Clone</a></li>
          <li><a href="rust-patterns-closure-traits.html">#9: Fn, FnMut, FnOnce</a></li>
          <li><a href="rust-patterns-storing-closures.html">#10: Storing and Returning Closures</a></li>
          <li><a href="rust-patterns-newtype.html">#11: Newtype</a></li>
          <li><a href="rust-patterns-typestate.html">#12: Typestate</a></li>
          <li><a href="rust-patterns-builder.html">#13: Builder Pattern</a></li>
          <li><a href="rust-patterns-enum-dispatch.html">#14: Enum Dispatch vs Trait Objects</a></li>
          <li><a href="rust-patterns-from-into.html">#15: From / Into Conversions</a></li>
          <li><a href="rust-patterns-error-handling.html">#16: Error Handling</a></li>
          <li><strong>#17: Arc&lt;Mutex&lt;T&gt;&gt; vs Arc&lt;RwLock&lt;T&gt;&gt;</strong></li>
          <li><a href="rust-patterns-channels.html">#18: Channels &mdash; Message Passing</a></li>
          <li><a href="rust-patterns-pin-futures.html">#19: Pin and Boxing Futures</a></li>
          <li><a href="rust-patterns-send-sync-async.html">#20: Send / Sync in Async</a></li>
        </ol>
      </nav>
    </article>
  </main>

  <footer>
    <div class="container">
      MIT &mdash; Copyright &copy; 2025 Mark Branion
    </div>
  </footer>
</body>
</html>
